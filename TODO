LinkedListRange
==================


ShouldBe_CheckOnce
For usages where majority of the time new value ranges that are added are
usually adjacent to one another and also having value ranges which are
non-overlapping and increasing, then ll_add_sorted_startfrom_lastadded is a
almost optimal logic to use, except for the situations where

the new value range to add doesnt belong to the end of the list, but somewhere
towards the begining or middle of the list, in this case, the
startfrom_lastadded will insert these to the location just before the lastadded
node.


Main logic
=============

* if a client is interruped during mcast transfer and then resumed into unicast
phase (by explicitly forcing the resume into unicast phase, because auto resume
will go back into mcast phase as required), then the client will have lost all
packets from the time of interruption till the end of mcast transfer. Currently
these lost packets can't be identified as lost, if the client is directly
forced into unicast phase. I have to add total number of packets involved in
the current transfer, as part of URReq packet from the server; and inturn in
the client use this info to update the lost packet ranges ll, if required.

* Multicast data packets in UCastRecovery (rather it becomes a kind of
MCastRecovery) phase.

Have implemented ll_delete_starthint_beforedel, which should help optimise the
deletion from lost packets list usage, if I were to reimplement the unicast
recovery as unicast query+response, but followed by multicast data packets
being sent towards the clients instead of the current unicast data packets to
the client which are working with at a given time in UR phase.

In normal ucast recovery, as the client recovering the lost data packets is
actively steering/controlling the server wrt what packets it will send, the
deletion of lost packet from the lost packets list, once it has been recieved,
will for the majority of the time occur at the beginning of the lost packets
list.

However if we allow the data packets sent to help with recovery for a given
client to be also multicast, so that it can help other clients which may have
lost the same packets, then other than the active client which is steering the
data packets seen on the network thro its URAckSeqNum response packets, all
other clients could be getting either packets which they already have with them
as well as they could be getting packets which they dont have, but these
packets could be somewhere other than the start of the lost packets list.

This ll_delete_starthint_beforedel helps handle this situation in a efficient
way.

* Also may be make the client side logic stateless.

i.e the client side logic should be written such that at the same time it is
listenning for either mcast data/stop phase packets or PI phase packets or UR
phase packets.

* Reduce the total time that server waits during ucast recover phase for client
to respond to its repeated URReq commands/packets.

Currently the server waits for 3 minutes with URReq packets sent every 10
seconds, to see if a given client will respond back with URAck response/packet.
Potentially we should reduce this to 1 or 1.5 minutes, so that unicast recovery
doesn't get dragged on by a long time for other clients which are active. While
at same time giving a sufficient time (1 min or so) for any given client to
respond back, with enough reminders in that time (repeat once every 10 secs).

This is done, but should I reduce this time further ????

* Client/Srvr resending pkts faster than required

If client which was quit during unicast recovery transfer either somewhere in
the middle of the transfer or during PI phase,

is resumed when the server is still in the middle of unicast recovery
transfers, then the PI packets sent by the resuming client, will trigger the
server to send out lot of URReq packets, which inturn the client corresponding
to that URReq will respond.  This leads to a large number of same URRes packets
with same data being recieved. And the server will currently inturn respond to
each of these repeated URRes packets, which leads to the same data packets
being sent to that client again and again.

is resumed when the server is still in the middle of PI phase, then everything
will be fine.

Have to modify the server side logic such that if it recieves PI packets in the
middle of UR transfer, it should still wait for the specified socket timeout to
see if a proper URRes comes or not, before resending the URReq packet again.

Even client side the PI logic resends PIReq packets faster than the configured
rate, if it recieves unrelated packets. Even this requires to be fixed.
