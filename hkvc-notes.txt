
v20181220
===========

hkvc-nw-test script new argument

--file file_to_send

Target java.net.multicast logic

Now it logs lost packets into lost.log file in the applications' directory on
external storage


v20181207IST1005
=================
hkvc-nw-test script arguments

--Bps 2000000 will set the throughput to 2M bytes per second

--datasize 1024 will set the packet size to 1K. The actual packet will be
4bytes+1K, where the 4 bytes correspond to 32bit seqNum in little-endian
format.

--dim 17, tells as to after how many packets are sent the throttling delay if
any should be applied.

--port 1111, tells that udp packets should be sent to port 1111

by default the logic is programmed to send packets to 127.0.0.1. By changing it
to a multicast ip address, one should be able to send to multicast groups
ideally. Have to cross-check the multicast packet sending requirements once,
but I feel that we dont require any special settting of socket for sending
multicast packets, while reception will require joining of the multicast group.
If this vague remembering of multicast behaviour that I have is correct, then
just changing the address in the program will allow using of this simple
pythong script to test multicast transfer behaviour to some extent.


v20181204
============

Now If only one mcast channel is being monitored, then it assumes that it could
be a high throughtput channel, so it will update the progress wrt monitoring in
the gui, only once every 10 iterations thro the monitoring loop.

However if more than 1 channel is being monitored at the same time, then as the
program currently doesn't provide a efficient way of handling this case, it
assumes that the channels are not high througput ones, and or the user is not
interested in getting accurate detailed monitored info like num of disjoint
seqNums noticied or num of times the seqNum jumped backwards etc. So it updates
the progress of monitoring in the GUI for each iteration thro the monitoring
loop.


v20181202
============

TODO1:

Verify if any buffering occurs if lot of packets are recieved on a given
channel.  Because in a given loop I read only 1 packet from a given channel and
wait for timeout or reception (again read only 1 packet, even if more are
there) of data on other channels.

And see the impact of the same practically.

NOTE1:

Supports max of 10 MCast channels i.e MCastGroupIP+Port.
It waits for upto 50msecs before timing out wrt each channel being monitored.
So if there are 10 channels being monitored and 9 of them don't have any data
then it will take 450+timeToReadDataFromTheSingleChannelWIthData msecs for 
each packet of data read from the alive channel.

So this will work for monitoring upto 10 channels with activitiy of 1 or 2
packets per second.

However if the data throughput is heavy, then monitor that single channel only 
to avoid lossing data packets due to overflow wrt buffers allocated by kernel
for the channel.

NOTE2:

ONe can specify different delay counts wrt when to treat delay in data activity
on a channel to be critical to mark it red. If only 1 channel is monitored,
then the delay count corresponds to delaycount*50msec of delay. However if more
than 1 channel is monitored, then the delay count to time mapping is more
complicated and dependent on data activity in realtime across all those
channels. Rather the delaycount can be treated as how many times the
applications checked to see if there is any data for a given channel and then
timedout.

TODO2: If I account timeout wrt other channels also, for each given channel,
then the delay count mirrors the actual time lost more accurately, and the 
delaycount*50msec can still be valid to a great extent. However the current
logic doesn't do this. Also this logic would assume that any channel which
reads data instead of leading to a timeout, will read the data at a very fast
rate which is in the vicinity of within a msec or so. Else the delta between
the actual delaycount based time calculation and real wall clock time will
increase.

