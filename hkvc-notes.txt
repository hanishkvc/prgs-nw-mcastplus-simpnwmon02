##################################
Nw Testing/Data transfer logic
v20190105IST2331
HanishKVC, 19XY
##################################


Logic and Usage
#################

Overview
==========

This set of programs helps verify network performance as well as if required
transfer a file from server to multiple clients.

The Phases
------------

It consits of

Multicast based transfer logic

Multicast based stop logic

Unicast based Presence Info logic - to help clients and server come to know
about one another.

Unicast based data / lost packet recovery logic - The server communicates with
the clients one by one and gets their list of lost packets and helps them
recover those by resending it thro unicast.

UnicastRecovery
-----------------
If a client stops responding in the middle of unicast error recovery or has
used up too many attempts and has still not fully recovered its lost packets,
then the server side logic will gracefully keep that client aside, and go to
the remaining clients. In turn at the end it will come back and check which
clients had been kept aside and then will try to help those clients recover
their lost packets.

Depending on the length of the content transfered, the server logic will decide
for how many times it should run thro the kept aside clients lists. Even after
that if there are clients which haven't fully recovered, the server will list
those clients and give up.

The logic will assume that upto a max of 8% to 10% packet losess could be there
and based on that decides how many attempts it should try wrt clients that keep
getting kept aside.

NOTE: If a client doesn't respond back to the server for upto 3 minutes or if
it has not recovered all its lost packets even after handshaking with it for
512 times, then it is kept aside temporarily.

When the client communicates with server with URAckSeqNum, it not only gives a
small list of lost packet ranges to recover, but also in total how many lost
packet ranges as well as inturn the total number of lost packets at that given
time in the client.

LinkedListRanges
-----------------

The linked list will maintain reference to the start, end as well as the last
added node in the list. It also maintains a count of total number of nodes and
inturn the total number of values stored in the list in the shortened form of
ranges.


Client
==========

The client side logic is implemented in a single program.

The client side simpnwmon02 program has the following command line arguments

./simpnwmon02 local_nwif_index mcast_ip local_nwif_ip data_file nw_bcast_ip

the local_nwif means the ethernet or wifi interface which connects to the
network on which we want to run the test/data transfer logic.

the local_nwif_index is the index assigned by linux kernel for the used network
interface. It can be got by using ip addr and looking at the index number
specified by it. i.e if it is the 1st nw interface or .... Nth network
interface for which address details are provided by ip addr command.

the local_nwif_ip is the ip address assigned to the network interface which we
want to use.

The local_nwif_index and local_nwif_ip are used as part of the multicast join
using setsockoption. Ideally one is required to provide only one of these two
values.

If local_nwif_index is not being explicitly specified, then pass 0 for it.

If local_nwif_ip is not explicitly specified, then pass 0.0.0.0 for it.


Server
========

The server side logic is implemented as part of two different programs.

The first takes care of the multicast phases.

The second takes care of the unicast phases.


Examples
==========


Example for actual testing
----------------------------

Client> ./simpnwmon02 0 230.0.0.1 10.0.2.11 /path/to/datafile 10.0.2.255

Server> ./hkvc-nw-send-mcast.py --addr 230.0.0.1 --file /path/to/file_to_send
Server> ./hkvc-nw-recover.py --file /path/to/file_to_send



Example for testing using VM
------------------------------

On Client run

Client> ./simpnwmon02 0 230.0.0.1 10.0.2.11 /dev/null 10.0.2.255

On Server run, these two commands one after the other

Server> ./hkvc-nw-send-mcast.py --addr 230.0.0.1 --testblocks 50000 --simloss
Server> ./hkvc-nw-recover.py --fast






Notes / Thoughts during some of the releases
#############################################


v20181228
===========

There is some odd holes seen in the data file after both mcast and ucast are
finished successfully. Need to cross-check this later.

Tried changing from FileOutputStream to RandomAccessFile in-case if its that
FileOutputStream doesn't allow selective writing into a existing file, but that
doesnt seem to have solved it, need to test the RandomAccessFile after removing
the data.bin file on the target and see how a fresh transfer with
RandomAccessFile works out.

Also on testing on a actual physical android target, found that if the packet
data size is at something like 8 bytes or so, the Android Java based GUI is
picking up the packets, but if I increase the data size to 32 or above, it
doesn't seem to be recieving the packets.

v20181225+
==========

The nw port usage is as follows

a) 1111 - Multicast Server to Clients data push
b) 1112 - Nw Broadcast PIReq from Client to Any listening Server
c) 1113 - Unicast PIAck from Server to Client


So if using Android AVD for testing remember to redir both 1111 as well as 1113

i.e telnet localhost 5554
NOTE: assuming it is the 1st avd started
auth value_required_to_authenticate
NOTE: got from .emulator.... file in the users home dir
redir add udp:1111:1111
redir add udp:1113:1113
redir list

Also if using AVD, then in GUI remember to set the PIInitialAddr to 10.0.2.255
in the given unicast related edittext.



v20181223
===========

Data Thread synchronisation
------------------------------

* Failure - UseData before FillData
Producer->Acquire->FillData->Loop
Consumer->UseData->Release->Loop

* Failure - Race, FillData before UseData is finished
Producer->FillData->Acquire->Loop
Consumer->Release->UseData->Loop

3Locks&Buffers
1,2,3,0-1
0,0,0,1=XXXXXX

* Ok - SemCount 1 or more less than Total Buffers
Producer->FillData->Acquire->Loop
Consumer->Release->UseData->Loop

3B(2L)
1-1,2-2,3-1,
0-0,1-1,2-2,

B1-L1,B2-L2,B3-L1
L0-B0,L0-B0,L1-B1

But will require dummy producing to flush out data in deltaOf(buf-lock) buffers
at the end, when actual producing is done.

TODO
-------

01) Currently Data is copied from a fixed buffer in AsyncTask to the data
buffer in DataHandler, avoid this and use the data buffer in DataHandler
directly.

02) Currently only a predefined (set to 1 currently) monitored channel is
logged as well as its data saved.

However if required Update the Logging and Data saving logic to work across
multiple channels.  i.e Each channels log and data should be saved to seperate
log and data files.

03) There is a issue with the 1st packet with seq number 0 being considred as a
olderSeqs, fix this corner case.

04) Add logic to use unicast to recover the packets lost during multicast.


v20181220
===========

hkvc-nw-test script new argument

--file file_to_send

Target java.net.multicast logic

Now it logs lost packets into lost.log file in the applications' directory on
external storage


v20181207IST1005
=================
hkvc-nw-test script arguments

--Bps 2000000 will set the throughput to 2M bytes per second

--datasize 1024 will set the packet size to 1K. The actual packet will be
4bytes+1K, where the 4 bytes correspond to 32bit seqNum in little-endian
format.

--dim 17, tells as to after how many packets are sent the throttling delay if
any should be applied.

--port 1111, tells that udp packets should be sent to port 1111

by default the logic is programmed to send packets to 127.0.0.1. By changing it
to a multicast ip address, one should be able to send to multicast groups
ideally. Have to cross-check the multicast packet sending requirements once,
but I feel that we dont require any special settting of socket for sending
multicast packets, while reception will require joining of the multicast group.
If this vague remembering of multicast behaviour that I have is correct, then
just changing the address in the program will allow using of this simple
pythong script to test multicast transfer behaviour to some extent.


v20181204
============

Now If only one mcast channel is being monitored, then it assumes that it could
be a high throughtput channel, so it will update the progress wrt monitoring in
the gui, only once every 10 iterations thro the monitoring loop.

However if more than 1 channel is being monitored at the same time, then as the
program currently doesn't provide a efficient way of handling this case, it
assumes that the channels are not high througput ones, and or the user is not
interested in getting accurate detailed monitored info like num of disjoint
seqNums noticied or num of times the seqNum jumped backwards etc. So it updates
the progress of monitoring in the GUI for each iteration thro the monitoring
loop.


v20181202
============

TODO1:

Verify if any buffering occurs if lot of packets are recieved on a given
channel.  Because in a given loop I read only 1 packet from a given channel and
wait for timeout or reception (again read only 1 packet, even if more are
there) of data on other channels.

And see the impact of the same practically.

NOTE1:

Supports max of 10 MCast channels i.e MCastGroupIP+Port.
It waits for upto 50msecs before timing out wrt each channel being monitored.
So if there are 10 channels being monitored and 9 of them don't have any data
then it will take 450+timeToReadDataFromTheSingleChannelWIthData msecs for 
each packet of data read from the alive channel.

So this will work for monitoring upto 10 channels with activitiy of 1 or 2
packets per second.

However if the data throughput is heavy, then monitor that single channel only 
to avoid lossing data packets due to overflow wrt buffers allocated by kernel
for the channel.

NOTE2:

ONe can specify different delay counts wrt when to treat delay in data activity
on a channel to be critical to mark it red. If only 1 channel is monitored,
then the delay count corresponds to delaycount*50msec of delay. However if more
than 1 channel is monitored, then the delay count to time mapping is more
complicated and dependent on data activity in realtime across all those
channels. Rather the delaycount can be treated as how many times the
applications checked to see if there is any data for a given channel and then
timedout.

TODO2: If I account timeout wrt other channels also, for each given channel,
then the delay count mirrors the actual time lost more accurately, and the 
delaycount*50msec can still be valid to a great extent. However the current
logic doesn't do this. Also this logic would assume that any channel which
reads data instead of leading to a timeout, will read the data at a very fast
rate which is in the vicinity of within a msec or so. Else the delta between
the actual delaycount based time calculation and real wall clock time will
increase.

